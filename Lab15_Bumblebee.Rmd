---
title: "Lab15_Bumblebee"
author: "William Brickowski, Rachael Robinson, Manasi Raol, Brady Kiesling"
date: "5/1/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Lab 15

## Data Links:
  * Kenneth French Data: http://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html#Research
  * Gapminder: 
https://www.gapminder.org/data/
  
  
## Team Question:
  * How are various industry portfolio returns related to various financial and macroeconomic factors?
  
## Domain expert

Our domain expert is a macroeconomics professor at CU. They are interested in knowing how businesses are affected by macroeconomic factors, and how we can make predictions based off of those factors. In this way, our domain expert would be very interested in the results of the industry return data we have decided to analyze.

## Why is this question important:
Reason: The factors that contribute to returns throughout various industries are something of hot debate within the financial industry; better understanding what leads to respective returns throught various industries will better allow economists understand the booms and busts within the economy and provides insight for asset managers who are tasked with asset allocation throughout different industries

## Team plot, answering question:

## Conclusions

## Recommendations


## Individual Investigation:

```{r include=FALSE}
library(tidyverse)
industry_returns=read_csv("10_Industry_Portfolios.csv")
```

### William

What is the question: visualizing historical industry returns & investigating the 3 factor Fama French regression analysis

Why is it important/interesting: Being able to visualize historical returns throughout various industries is an essential way to recognize patterns in the market so as to understand how various events within particular time periods may effect returns throughout different industries. The Fama French nobel prize winning 3 factor model is the current model used in finance academia to predict industry returns. Returns are the response in the regression anaylsis and there are three predictors. The three predictors in the model are market returns minus the risk free rate, a size premium, and a value premium. Being able to build a predictive model is very benefical as it can be used to forecast future returns. The model can also be explanatory to explain what factors are related to the returns in various industries.

```{r}
factor_returns=read_csv("5_Factor_Returns.csv")
aggregate = left_join(industry_returns, factor_returns, by="Year")
aggregate = aggregate %>% na.omit()

aggregate$Year = parse_date(aggregate$Year,format="%Y%m")

ggplot(data = aggregate) + 
  geom_line(aes(x = Year, y = NoDur, color='skyblue')) +                         
  xlab('Years') +                                              
  ylab('NoDur industry return') 

ggplot(data = aggregate) + 
  geom_line(aes(x = Year, y = Durbl, color='skyblue')) +                         
  xlab('Years') +                                              
  ylab('Durbl industry returns') 

ggplot(data = aggregate) + 
  geom_line(aes(x = Year, y = Manuf, color='skyblue')) +                         
  xlab('Years') +                                              
  ylab('Manuf industry returns') 

ggplot(data = aggregate) + 
  geom_line(aes(x = Year, y = Enrgy, color='skyblue')) +                         
  xlab('Years') +                                              
  ylab('Enrgy industry returns') 

ggplot(data = aggregate) + 
  geom_line(aes(x = Year, y = HiTec, color='skyblue')) +                         
  xlab('Years') +                                              
  ylab('HiTec industry returns')

ggplot(data = aggregate) + 
  geom_line(aes(x = Year, y = Telcm, color='skyblue')) +                         
  xlab('Years') +                                              
  ylab('Telcm industry returns')

ggplot(data = aggregate) + 
  geom_line(aes(x = Year, y = Shops, color='skyblue')) +                         
  xlab('Years') +                                              
  ylab('Shops industry returns')

ggplot(data = aggregate) + 
  geom_line(aes(x = Year, y = Hlth, color='skyblue')) +                         
  xlab('Years') +                                              
  ylab('Hlth industry returns')

ggplot(data = aggregate) + 
  geom_line(aes(x = Year, y = Utils, color='skyblue')) +                         
  xlab('Years') +                                              
  ylab('Utils industry returns')

ggplot(data = aggregate) + 
  geom_line(aes(x = Year, y = Other, color='skyblue')) +                         
  xlab('Years') +                                              
  ylab('Other industry returns')

lmod_NoDur = lm(NoDur ~ `Mkt-RF`+`SMB`+`HML`+`RMW`+`CMA`, data=aggregate)
lmod_Durbl = lm(Durbl ~ `Mkt-RF`+`SMB`+`HML`+`RMW`+`CMA`, data=aggregate)
lmod_Manuf = lm(Manuf ~ `Mkt-RF`+`SMB`+`HML`+`RMW`+`CMA`, data=aggregate)
lmod_Enrgy = lm(Enrgy ~ `Mkt-RF`+`SMB`+`HML`+`RMW`+`CMA`, data=aggregate)
lmod_HiTec = lm(HiTec ~ `Mkt-RF`+`SMB`+`HML`+`RMW`+`CMA`, data=aggregate)
lmod_Telcm = lm(Telcm ~ `Mkt-RF`+`SMB`+`HML`+`RMW`+`CMA`, data=aggregate)
lmod_Shops = lm(Shops ~ `Mkt-RF`+`SMB`+`HML`+`RMW`+`CMA`, data=aggregate)
lmod_Hlth = lm(Hlth ~ `Mkt-RF`+`SMB`+`HML`+`RMW`+`CMA`, data=aggregate)
lmod_Utils = lm(Utils ~ `Mkt-RF`+`SMB`+`HML`+`RMW`+`CMA`, data=aggregate)
lmod_Other = lm(Other ~ `Mkt-RF`+`SMB`+`HML`+`RMW`+`CMA`, data=aggregate)

summary(lmod_NoDur)
summary(lmod_Durbl)
summary(lmod_Manuf)
summary(lmod_Enrgy)
summary(lmod_HiTec)
summary(lmod_Telcm)
summary(lmod_Shops)
summary(lmod_Hlth)
summary(lmod_Utils)
summary(lmod_Other)

aggregate = aggregate %>% mutate(year_only = format(aggregate$Year,"%Y"))
pre2013 = aggregate %>% filter(year_only<2013)
post2013 = aggregate %>% filter(year_only>=2013)

lmod_NoDur_pre2013 = lm(NoDur ~ `Mkt-RF`+`SMB`+`HML`+`RMW`+`CMA`, data=pre2013)
lmod_Durbl_pre2013 = lm(Durbl ~ `Mkt-RF`+`SMB`+`HML`+`RMW`+`CMA`, data=pre2013)
lmod_Manuf_pre2013 = lm(Manuf ~ `Mkt-RF`+`SMB`+`HML`+`RMW`+`CMA`, data=pre2013)
lmod_Enrgy_pre2013 = lm(Enrgy ~ `Mkt-RF`+`SMB`+`HML`+`RMW`+`CMA`, data=pre2013)
lmod_HiTec_pre2013 = lm(HiTec ~ `Mkt-RF`+`SMB`+`HML`+`RMW`+`CMA`, data=pre2013)
lmod_Telcm_pre2013 = lm(Telcm ~ `Mkt-RF`+`SMB`+`HML`+`RMW`+`CMA`, data=pre2013)
lmod_Shops_pre2013 = lm(Shops ~ `Mkt-RF`+`SMB`+`HML`+`RMW`+`CMA`, data=pre2013)
lmod_Hlth_pre2013 = lm(Hlth ~ `Mkt-RF`+`SMB`+`HML`+`RMW`+`CMA`, data=pre2013)
lmod_Utils_pre2013 = lm(Utils ~ `Mkt-RF`+`SMB`+`HML`+`RMW`+`CMA`, data=pre2013)
lmod_Other_pre2013 = lm(Other ~ `Mkt-RF`+`SMB`+`HML`+`RMW`+`CMA`, data=pre2013)

lmod_NoDur_post2013 = lm(NoDur ~ `Mkt-RF`+`SMB`+`HML`+`RMW`+`CMA`, data=post2013)
lmod_Durbl_post2013 = lm(Durbl ~ `Mkt-RF`+`SMB`+`HML`+`RMW`+`CMA`, data=post2013)
lmod_Manuf_post2013 = lm(Manuf ~ `Mkt-RF`+`SMB`+`HML`+`RMW`+`CMA`, data=post2013)
lmod_Enrgy_post2013 = lm(Enrgy ~ `Mkt-RF`+`SMB`+`HML`+`RMW`+`CMA`, data=post2013)
lmod_HiTec_post2013 = lm(HiTec ~ `Mkt-RF`+`SMB`+`HML`+`RMW`+`CMA`, data=post2013)
lmod_Telcm_post2013 = lm(Telcm ~ `Mkt-RF`+`SMB`+`HML`+`RMW`+`CMA`, data=post2013)
lmod_Shops_post2013 = lm(Shops ~ `Mkt-RF`+`SMB`+`HML`+`RMW`+`CMA`, data=post2013)
lmod_Hlth_post2013 = lm(Hlth ~ `Mkt-RF`+`SMB`+`HML`+`RMW`+`CMA`, data=post2013)
lmod_Utils_post2013 = lm(Utils ~ `Mkt-RF`+`SMB`+`HML`+`RMW`+`CMA`, data=post2013)
lmod_Other_post2013 = lm(Other ~ `Mkt-RF`+`SMB`+`HML`+`RMW`+`CMA`, data=post2013)

cat("The difference in R-Squared for NoDur pre2013 linear model and post 2013 is:", summary(lmod_NoDur_pre2013)$r.squared-summary(lmod_NoDur_post2013)$r.squared)

cat("The difference in R-Squared for Durbl pre2013 linear model and post 2013 is:",
summary(lmod_Durbl_pre2013)$r.squared-summary(lmod_Durbl_post2013)$r.squared)

cat("The difference in R-Squared for Manuf pre2013 linear model and post 2013 is:",
summary(lmod_Manuf_pre2013)$r.squared-summary(lmod_Manuf_post2013)$r.squared)

cat("The difference in R-Squared for Enrgy pre2013 linear model and post 2013 is:",
summary(lmod_Enrgy_pre2013)$r.squared-summary(lmod_Enrgy_post2013)$r.squared)

cat("The difference in R-Squared for HiTec pre2013 linear model and post 2013 is:",
summary(lmod_HiTec_pre2013)$r.squared-summary(lmod_HiTec_post2013)$r.squared)

cat("The difference in R-Squared for Telcm pre2013 linear model and post 2013 is:",
summary(lmod_Telcm_pre2013)$r.squared-summary(lmod_Telcm_post2013)$r.squared)

cat("The difference in R-Squared for Shops pre2013 linear model and post 2013 is:",
summary(lmod_Shops_pre2013)$r.squared-summary(lmod_Shops_post2013)$r.squared)

cat("The difference in R-Squared for Hlth pre2013 linear model and post 2013 is:",
summary(lmod_Hlth_pre2013)$r.squared-summary(lmod_Hlth_post2013)$r.squared)

cat("The difference in R-Squared for Utils pre2013 linear model and post 2013 is:",
summary(lmod_Utils_pre2013)$r.squared-summary(lmod_Utils_post2013)$r.squared)

cat("The difference in R-Squared for Other pre2013 linear model and post 2013 is:",
summary(lmod_Other_pre2013)$r.squared-summary(lmod_Other_post2013)$r.squared)

```

What new tools did you use, what is the answer to the question, how does this relate to the overall question: In order to perform my research, I had to my two data sets which were historical industry returns as well as the three factors of the fama french model historically, as well. I used the left_join function to join my two tibbles nd then used the na.omit() function to remove any rows that contain NA values. I then use the parse_date function to convert the year column to date type so that it can be graphed as a time series. This is followed with the ggplot function being called to show the historical returns throughout all of the industries graphically. I then apply what we learned about models to build a linear model for each industries returns using the three predictors laid out by Fama and French in there 2013 nobel prize winning paper. I call the summary of the linear model objects to better understand which predictors are significant and at what level as well as understanding how a unit change in the predictor changes the returns in a respective industry. In the final part of my section, I use the mutate function to create a new column which extract just the year from the date column. I do this so that I can use the pip to filter my data into two new tibbles: one for the years preceding 2013 and one for the years after it. I do this because I wanted to see if 2013 which is when Fama and French recieved their nobel prize, effects the prediction success of the linear models. I build out a model using the pre and post 2013 data for all industries and then take the difference in R-Squared. In no case was the R-Squared value larger after 2013 which suggests that the releasing of the paper did not have an effect on describing returns meaning the market wasn't biased towards this paper. This is a good result and indicates that the work of Fama and French truly did describe a relationship inherent in the market, regardless of how widely accepted their academic work was. From the original graphs that were generated it is clear that all market returns have grown over time. Now, using the Fama French Model, we can better predict market returns and even potentially predict economic cycles.

### Rachael

Question: How do the number of total patents correlate to growth/decay in returns for different industry sectors?

Why this questions is important/interesting: In this lab, we are attempting to understand the factors which affect industry returns. From the gapminder dataset, we are able to access the patent counts across the years. Patents are typically associated with tech and new inventions, so it would be interesting to determine how returns actually correlate to number of patents. Intuitively, we would expect more patents to result in higher returns, but discovering where the discrepencies lie can help understand if patent counts are an important factor in industry returns.

To answer this question, I tidied the datasets and plotted all the industry sectors as a function of patent counts.

```{r}
# reading in data
library(tidyverse)
industry <- read.csv("10_Industry_Portfolios.csv", stringsAsFactors=FALSE)
patents <- read.csv("patents_granted_total.csv")

# tidying the patent dataset

patents <- patents %>%
  filter(country == "United States")

gathernames <- patents %>%
  select(-country) %>%
  colnames()

patents <- patents %>%
  gather(gathernames, key = "year", value = patent_count)

patents <- patents %>%
  select(-country)

patents$year <- parse_number(patents$year)

# patent dataset tidy


# tidying the industry dataset

# note that the industry dataset is actually several different datasets

# The data set I will need is Average Value Weighted Returns - Annual, because my patent data is annual as well

# These are rows 2235-2326 in dataset

industry <- industry[c(2235:2326),]

industry <- industry %>%
  rename(year = Year)

industry$year <- parse_number(industry$year)

# dataset is as needed now


# joining the industry and patent datasets
patently <- inner_join(industry, patents, by = "year")

# patently is dataset from which visualizations and inferences will come

patently[, c(2:11)] <- sapply(patently[, c(2:11)], as.numeric)


ggplot(data = patently) +
  geom_smooth(se = FALSE, mapping = aes(x=patent_count, y=HiTec, color = "HiTec")) +
  geom_smooth(se = FALSE, mapping = aes(x=patent_count, y=NoDur, color = "NoDur")) +
  geom_smooth(se = FALSE, mapping = aes(x=patent_count, y=Durbl, color = "Durbl")) +
  geom_smooth(se = FALSE, mapping = aes(x=patent_count, y=Manuf, color = "Manuf")) +
  geom_smooth(se = FALSE, mapping = aes(x=patent_count, y=Enrgy, color = "Enrgy")) +
  geom_smooth(se = FALSE, mapping = aes(x=patent_count, y=Telcm, color = "Telcm")) +
  geom_smooth(se = FALSE, mapping = aes(x=patent_count, y=Shops, color = "Shops")) +
  geom_smooth(se = FALSE, mapping = aes(x=patent_count, y=Hlth, color = "Hlth")) +
  geom_smooth(se = FALSE, mapping = aes(x=patent_count, y=Utils, color = "Utils")) +
  geom_smooth(se = FALSE, mapping = aes(x=patent_count, y=Other, color = "Other")) +
  labs(title = "Industry sector returns across patent count", x = "Patent count", y = "Returns")
```

The graph indicates that the more extreme cases may be in the HiTec and NoDur industries, with the HiTec showing growth in increasing patent numbers, and the NoDur showing decay with increasing patent numbers. We will now employ two correlation tests: one for the correlation between patents and consumer Non-durables returns, and one for correlation between patents and high tech returns.


```{r}
perm_cor <- function(perms = 1000, x, y)
{
  ## Variables ##
  # perms: The number of permutations 
  # x: Vector 1 - for computing correlation
  # y: Vector 2 - for computing correlation
  ###############
  
  # Step 1:
  # Create vector of zeroes of length "perms" to store
  # permuted mean differnces
  
  corr_result <- vector("double", length = perms)
  
  # Loop throught number of permutations
  for (i in c(1:perms))
  {
    # Step 2:
    # Randomly mix up the values in the vector "y"
    
    y <- sample(y)
    
    # Step 3:
    # Compute the correlation between x and the randomly mixed
    # up y-vector. Store this value in the vector from step 1.
    
    corr_result[i] <- cor(x,y)
  }
  
  # Step 4:
  # Return new updated vector, created in step 1
  return(corr_result)
}

corr_hitec <- perm_cor(perms = 1000, patently$patent_count, patently$HiTec)

corr_nodur <- perm_cor(perms = 1000, patently$patent_count, patently$NoDur)

hitec_corr <- cor(patently$patent_count, patently$HiTec)
nodur_corr <- cor(patently$patent_count, patently$NoDur)
ggplot() +
  geom_histogram(mapping = aes(x=corr_hitec)) +
  geom_vline(mapping = aes(xintercept = hitec_corr, color = "Original sample correlation")) +
  labs(title = "Correlation permutation test for patents and high tech returns", subtitle = "1000 correlation test, red line represents true correlation between patents and high tech returns", x = "Correlation")

```

The preceeding plot shows the correlation distribution and true correlation of patent counts and high tech returns. According the plot, the true correlation is near zero and falls nearly in the middle of the bell curve, indicating that we fail to reject the null hypothesis that the correlation between patent counts and high tech returns is near 0. The following percentile calculation affirms this notion.

```{r}
percentite_hitec <- sum(corr_hitec<hitec_corr)/1000

print(percentite_hitec)
```

```{r}
ggplot() +
  geom_histogram(mapping = aes(x=corr_nodur)) +
  geom_vline(mapping = aes(xintercept = nodur_corr, color = "Original sample correlation")) +
  labs(title = "Correlation permutation test for patents and Non-durables", subtitle = "1000 correlation test, red line represents true correlation between patents and non-durables", x = "Correlation")
```

The plot above shows the correlation distribution between number of patents and returns on non-durables. In this case, we see that the true correlation in the dataset is far on the tails of the distribution, indicating that the true correlation between number of patents and non-durable returns is non-zero, and we reject the null hypothesis. This, again, can be affirmed at the 0.05 significance level with a percentile calculation.

```{r}
percentite_nodur <- sum(corr_nodur<nodur_corr)/1000

print(percentite_nodur)
```

This calculation shows statistical significance at the 0.05 level, so we reject the null hypothesis and assume that the true correlation between non-durables and patent counts in non-zero, and the graph indicates this correlation is negative.

New tools to answer questions: To answer this question, I first had to manipulate and tidy the data to make sure I had the observations necessary to answer this questions. In order to do this, I had to use the sapply function to cast the returns as numeric values. In addition, to answer the statistical questions at hand, I used the new technique of the permuted correlation test, as well as percentiles to justify my answers and conclusions.

Answer to question: Through this analysis, we observe a non-significant relationship between patent counts and high tech industry returns, but a significant and negative correlation between patent counts and non-durable industry returns. This answer seems intuitively reasonable because patents are typically not associated with the non-durable industry like food, tobacco, etc. Thus, over time, the more patents granted would presumably benefit other industries, leaving returns on non-durables lower. However, the data did not show directly that patents are associated with high returns in tech. We could not find a significant relationship between patent count and high tech returns, indicating that other variables and industry sectors may be at play.

How does this relate to overall question: Patents are a crucial part of innovation and development in many industries observed in this dataset. Understanding how patents can influence returns could allow us to make recommendations on their use to improve returns. Understanding patents as an economic indicator can help us further tease out the relationship between macroeconomic factors and industry returns.


### Manasi

### Brady


## Reflection on Lab 2:

### Teamâ€™s goals

From Lab 2:

Learning to use R, applying statistics to real world problems, and learning how to use GitHub

### How well they were achieved

Throughout this course, we feel we have gained many skills in R. Our team feels that in this course, we have overwhelmingly achieved one goal of learning how to visualize data, given how many times we practiced it throughout the course.

We also used statistical analysis like the permutation tests and percentiles to answer unique statistical questions, which is something our team could not have done at the beginning of the semester. Knowing we have completed at least 3 labs where statistical analysis was necessary to real data (baby data, gapminder, etc), we believe we have achieved this goal. However, we also believe that there are many more statistical tests and inferences that we have yet to learn, so our goal will continue into other classes and semesters.

Finally, our last goal was learning how to use GitHub. We used GitHub to share our work for the whole semester, but we had some trouble navigating the nuances of GitHub, and particularly GitKraken. Thus, we have met this goal moderately well. With more time, we could learn more about how to make GitKraken more user-friendly for our team.

### One thing you would tell your team to keep doing? Stop doing? Start doing? 

Keep doing - If we could travel back in time, we would tell our team to keep coming prepared to lecture and lab. We rarely had issues with preparedness when it came time for the iRAT and tRAT, so we would want our team to keep reading the modules so that we can do well on the assessments.

Stop doing - One thing we would tell our team is to stop working individually when possible. At the beginning, our labs suffered from lack of cohesion, so we would tell our team to look at and contribute to the lab with other team member's contributions in mind.

Start doing - If we could speak to our team at the beginning of the semester, we would tell them to start working on the labs earlier in the week. We often experienced tight time crunches before due dates, so starting the labs earlier would help keep our time management on track and get more complete labs turned in earlier.

## Individual Reflections:

### William

Over the course of this semester, I believe some of my goals have become ever more prominent in my life while I have realized past directions may not be where I actually want to go at this point. I set out to learn R in this course and my expectations of succeeding at this goal were surpassed. I have used much of what I have learned in this class to model math problems in other courses such as Markov Processes as well as build out linear models in my Applied Regression course. I also wanted to learn data science techniques and in reading along with this courses text, applying what we learned in projects and on in class exercises, I believe I have a good overall understanding of the process from data exploration to data tyding to building models and making conclusions. Going into this classes, if you had asked me, I would have told you that I would have liked to apply these skills into the financial sectors - trading in particular. However, after watching the gapminder TED talk, I have realized that there are so many other amazing applications of data science. This course has sparked my interest in exploring all possible paths. Some advice that I would tell myself would be that completing the coding exercises in the reading is very helpful. Also working out a good system with your team in the beginning, which we did, is essential. Also to prevent merge conflicts set up strict protocal with your team. Great semester overall.

### Rachael

My original 6 month after graduation goal was to have a job in industry where I could use statistical analyses to answer questions. This goal has not changed at all, and this summer I will be working as a data analyst intern at a start-up. My goal immediately after graduation is still to get a job in the data science industry. My five year after graduation goal is to start integrating statistics and writing to understand people's perceptions of numbers. This is still my professional goal.

In this course, I learned the basics of R, coding semantics, and a ton of different techniques to answer questions. In this course, I learned how to visualize and tidy data. This learning has already extended to my other courses, and I have accomplished statistical inference on real data. These were all things I could not do before taking this class.

If I could give myself advice at the beginning of the semester, I would tell myself to keep thinking outside the box in the labs. I learned in the course that creativity is king, so I would tell myself to keep trusting myself and answer the questions that I myself were interested in answering.

I would also tell myself to stop unfocusing after my portion of the lab was turned in. Being more present throughout the whole lab process would probably have helped my team and myself avoid mistakes in the labs. Maintaining enegy throughout the week would help our end products.

Finally, I would tell myself to start asking more clarifying questions about the lab. Sometimes the labs were ambiguous in technique and approach, so reaching out to professors, TAs, and other teams might have helped answer our questions better and faster.

### Manasi

### Brady 


## Who did what:

### William

### Brady

### Manasi

### Rachael

For this lab, I created and answered my own subquestion with new techniques we have learned in class. For my subquestion, I imported data, tidied two datasets, joined them, visualized them, and then completed statistical tests and reported statistical inferences. Finally, I made conclusions based on those inferences. Additionally, I wrote about our domain expert for the team question, as well as writing up the reflections on our team performance throughout the semester.